Strategic Blueprint for Winning Phase 1 of AgentX AgentBeats: The Jurist-Bench Architecture1. Executive Summary and Competition AlignmentThe rapid evolution of Artificial Intelligence has precipitated a shift from static Large Language Models (LLMs) to dynamic, goal-oriented Agentic AI. This transition fundamentally breaks existing evaluation paradigms. Static benchmarks, which rely on multiple-choice questions or fixed-text summarization, fail to capture the defining characteristics of agents: multi-step planning, tool usage, environmental interaction, and error recovery. The Berkeley RDI AgentX AgentBeats Competition represents a critical intervention in this field, seeking to establish the rigorous, standardized infrastructure required to measure the next generation of AI systems.1This report serves as a comprehensive strategic roadmap for participating in and winning Phase 1 of the competition. The user’s intent to utilize the LAiW (Legal AI in the Wild) dataset 3 and the qwen/qwen3-4b-thinking-2507 model 4 aligns powerfully with the competition’s objectives, provided these assets are deployed within the correct architectural framework.1.1 Verification of Phase 1 GoalsA forensic analysis of the competition documentation confirms that the primary objective of Phase 1 is unequivocally the creation of a "Green Agent".2 The competition distinguishes between two roles:Green Agents (Evaluators): These are the proctors, judges, and environment managers. They define the task, orchestrate the interaction, and automate the scoring.Purple Agents (Competitors): These are the test subjects that attempt to solve the tasks defined by the Green Agent.The rules state: "Phase 1: Each team designs 'green agents' for evaluating different task".1 While participants must also submit a baseline Purple Agent to demonstrate the functionality of their benchmark, the product being judged is the Green Agent itself. The strategic imperative, therefore, is to shift focus from "building a lawyer agent" to "building a digital court system."1.2 The Proposed Solution: Jurist-BenchTo secure a winning position, this report proposes "Jurist-Bench," a novel Green Agent architecture. Jurist-Bench transforms the static text pairs of the LAiW dataset into a stateful, interactive simulation. It adheres strictly to the competition's Agent-to-Agent (A2A) protocol 5, acting as a server that issues complex legal challenges to the competing Purple Agent (powered by qwen/qwen3-4b-thinking-2507).Jurist-Bench addresses the specific judging criteria of Benchmark Design Quality and Innovation 2 by moving beyond simple verdict prediction. Instead, it evaluates the process of legal reasoning: the discovery of evidence, the citation of statutes, and the construction of syllogistic arguments. This aligns with the "Safety" track sponsored by Lambda 2 by explicitly penalizing hallucination and enforcing rigorous grounding in the provided case files.2. Theoretical Framework: The Shift to Agentic EvaluationTo design a winning Green Agent, one must first understand the theoretical gap it fills. Current evaluation methods are insufficient for the agentic era, creating a "market failure" in AI research that AgentBeats aims to correct.2.1 The Limitations of Static Evaluation in LawTraditional legal benchmarks, including the raw version of LAiW, rely on static input-output pairs. A model is given a case description and asked to predict the charge or the prison term.3 This measures Semantic Pattern Matching—the ability of the model to correlate specific fact patterns with specific labels.However, the practice of law is not merely pattern matching; it is an Agentic Process. It involves:Information Asymmetry: A lawyer rarely starts with all the facts. They must interview witnesses and request documents.Tool Use: Lawyers must use external databases (Westlaw, LexisNexis) to find relevant statutes and precedents.Iterative Reasoning: Legal arguments are constructed over time, often requiring the refinement of a hypothesis based on new evidence.Static benchmarks fail to measure these capabilities. A model might correctly guess "Guilty" based on the phrase "stole a car," but fail completely if asked to "find the specific statute that defines theft in this jurisdiction." The "Jurist-Bench" Green Agent addresses this by simulating the Information Gap, releasing facts only when the Purple Agent explicitly asks for them via tool calls.2.2 The AgentBeats Ecosystem ArchitectureThe AgentBeats platform introduces a standardized architecture for this type of evaluation, centered on the Agent-to-Agent (A2A) Protocol.5ComponentFunction within AgentBeatsRole in Jurist-BenchThe PlatformThe central orchestrator that manages agent registration, discovery, and the master clock.Registers Jurist-Bench as a "Legal Evaluator" via an Agent Card.1The Green AgentThe "Host" of the specific benchmark. It runs in a Docker container and acts as a server.The Jurist-Bench server. It holds the LAiW database and the "Legal-F1" scoring logic.2The Purple AgentThe "Guest" or participant. It connects to the Green Agent to receive tasks.The qwen/qwen3-4b-thinking-2507 Baseline Agent. It demonstrates that the tasks are solvable.2A2A ProtocolThe communication standard (JSON-RPC over HTTP) allowing agents to talk.The language used for "Discovery" (getting evidence) and "Submission" (filing motions).62.3 The Imperative of StandardizationThe competition documentation highlights "Lack of Standardization" as a primary challenge.1 Agents often require custom environments, manual tweaking, and specific APIs to run. By enforcing the A2A protocol, AgentBeats ensures Interoperability. A winning Green Agent must be a "Good Citizen" of this ecosystem, implementing the A2A standard flawlessly so that any legal agent (not just the user's qwen/qwen3-4b-thinking-2507 agent) can connect and be evaluated. This "Plug-and-Play" capability is a major factor in the Reproducibility judging criterion.23. The Data Asset: Deep Analysis of LAiWThe user has selected the LAiW (Legal AI in the Wild) dataset.3 This choice is strategically sound. While English legal benchmarks like LegalBench 7 exist, the Chinese legal domain offers unique complexity due to its Civil Law structure, which relies heavily on statutory interpretation rather than case law precedent. This requires a rigorous "Logic-First" approach, fitting for an agentic benchmark.3.1 Dataset Composition and SelectionLAiW is not a single dataset but a collection of 14 foundational tasks organized into three capability levels.3 To build a high-complexity benchmark, we must filter these tasks rigorously.3.1.1 Level 1: Basic Information Retrieval (BIR)Tasks: Named Entity Recognition (NER), Element Recognition, Article Recommendation.Analysis: These are standard NLP tasks. While useful for pre-training, they are too simple for an Agentic competition. An agent that simply extracts entities is not demonstrating "Agency."Decision: Exclude from the primary evaluation loop, but use as auxiliary data for the scoring engine (to verify if the agent found the right entities).3.1.2 Level 2: Legal Foundation Inference (LFI)Tasks: Similar Case Matching, Charge Prediction, Prison Term Prediction.Analysis: These tasks begin to test reasoning but are often framed as classification problems.Decision: Partial Inclusion. We will use "Similar Case Matching" as a sub-task. The Purple Agent may need to find a similar case to support its argument, testing its Retrieval Augmented Generation (RAG) capabilities.3.1.3 Level 3: Complex Legal Application (CLA)Tasks: Judicial Reasoning Generation (JRG), Case Understanding (CU), Legal Consultation (LC).Analysis: This is the "Gold Mine."JRG (AC-NLG Dataset): Requires generating a text explaining why a verdict was reached.3 This tests the Legal Syllogism (Major Premise -> Minor Premise -> Conclusion).Legal Consultation (CrimeKgAssistant): Mimics a dialogue. This fits the multi-turn nature of agents perfectly.Decision: Primary Focus. Jurist-Bench will be built primarily on the CLA tasks. The Green Agent will task the Purple Agent with generating judicial reasoning based on disjointed facts.3.2 The Transformation Pipeline: From Static to AgenticTo win, we cannot simply serve the LAiW JSON files. We must transform them into Information States.3.2.1 The "Case Object" SchemaThe raw LAiW data links facts directly to verdicts. We will restructure this into a tiered object:Public State: The initial police report or plaintiff complaint. This is visible to the Purple Agent at Turn 0.Private State (The "Hidden Files"):Witness Statements: Available only via tool.interview_witness().Forensic Reports: Available only via tool.request_document().Statutes: Available only via tool.search_law().Ground Truth: The final verdict and reasoning logic (from the AC-NLG dataset). This is never shown to the Purple Agent; it is used solely by the Green Agent's scoring engine.3.2.2 The Ontology of Legal SyllogismChinese law follows a strict syllogistic structure. The Green Agent will explicitly evaluate this.Major Premise: The Law (e.g., Article 264 of Criminal Law).Minor Premise: The Facts (e.g., "Defendant took the wallet").Conclusion: The Verdict (e.g., "Theft").The LAiW dataset contains annotations for these elements.3 Jurist-Bench will parse these annotations to create a Structured Rubric. If the Purple Agent creates a conclusion without citing the Major Premise (The Law), it fails the "Grounding" check, regardless of whether the verdict is correct.4. The Computational Engine: qwen/qwen3-4b-thinking-2507The competition requires a "Baseline Purple Agent" to validate the Green Agent.2 The user's selection of qwen/qwen3-4b-thinking-2507 is optimal for this purpose. qwen/qwen3-4b-thinking-2507 represents the state-of-the-art in open-weights models for reasoning and tool use.44.1 The "Thinking" Advantageqwen/qwen3-4b-thinking-2507 introduces a dual-mode architecture: "Thinking" and "Non-Thinking".10Non-Thinking Mode: Standard fast generation. Good for chat.Thinking Mode: Uses a "scratchpad" approach to generate reasoning traces before the final output. This mimics the System 2 thinking required for law.Strategy: The Purple Agent will permanently enable enable_thinking=True with Temperature=0.6.12 This allows the agent to Plan its investigation.Internal Monologue: "I have the police report. It mentions a witness, Mr. Zhang. I do not have Mr. Zhang's statement. I should use the interview_witness tool to get it before I make a decision."This internal planning capability is crucial for solving the "Information Gap" introduced by Jurist-Bench. A standard LLM might hallucinate Mr. Zhang's testimony; qwen/qwen3-4b-thinking-2507 (in Thinking Mode) is more likely to realize the information is missing and ask for it.4.2 Mixture-of-Experts (MoE) EfficiencyWhile the 8B model is "Dense" 10, the qwen/qwen3-4b-thinking-2507 family's architecture allows for high performance with lower compute. This ensures that the Purple Agent can be run easily by the competition judges. A massive 70B model might cause timeout issues or require specialized hardware that complicates the Reproducibility judging criteria.2 The 8B model strikes the perfect balance: smart enough to solve the legal reasoning tasks, but light enough to run in a standard Docker container alongside the Green Agent.4.3 Tool Use Capabilitiesqwen/qwen3-4b-thinking-2507 has been optimized for Tool Use and Function Calling.12 The Qwen-Agent framework 14 provides a Python wrapper that translates natural language intent into structured JSON tool calls.Feature: Parallel Function Calling.Application: The Purple Agent can request multiple documents simultaneously ("Get the autopsy report AND the witness statement"), simulating a real lawyer's workflow and improving efficiency scores.Performance: Benchmarks show qwen/qwen3-4b-thinking-2507-8B executes tool calls with textbook-level logic, avoiding common errors like hallucinating arguments.135. System Architecture: The Jurist-Bench Green AgentThis section outlines the technical specifications for the product being submitted. Jurist-Bench is designed to be a "Reference Implementation" for the AgentBeats A2A protocol in the legal domain.5.1 The Architecture of the EvaluatorThe Green Agent is not a simple script; it is a Microservice. It must be robust, persistent, and stateless between runs.5.1.1 The Docker ContainerThe entire Green Agent runs within a single Docker image.2Base Image: python:3.10-slimDatabase: SQLite (embedded). We migrate the selected LAiW JSON data into a lightweight SQL database within the container. This ensures fast retrieval of "hidden" evidence without loading gigabytes of JSON into memory.Server: FastAPI + Uvicorn. This handles the HTTP/JSON-RPC requests required by the A2A protocol.5.1.2 The State Machine (The "Dungeon Master")The core logic of Jurist-Bench is a Finite State Machine (FSM).State 0: Handshake. The Purple Agent connects and presents its credentials.State 1: Discovery Phase.The Green Agent presents the Initial_Task (Police Report).The Green Agent enters a loop, waiting for Tool_Requests.Logic: If the Purple Agent requests get_statute("Article 264"), the State Machine queries the SQLite DB and returns the text. It decrements the "Turn Budget."State 2: Submission Phase.The Purple Agent signals readiness.The Green Agent accepts the final Legal_Motion text.State 3: Judgment Phase.The Green Agent runs the scoring pipeline.The Green Agent returns a Task_Result object via A2A.5.2 Implementing the A2A ProtocolTo win "Technical Correctness," the implementation of A2A must be flawless. We will adhere to the A2A Specification Layer 3 (Protocol Bindings).155.2.1 Agent Discovery: The Agent CardJurist-Bench will publish a dynamic Agent Card at the /agent-card endpoint.JSON{
  "name": "Jurist-Bench-Green",
  "description": "Evaluator for Chinese Legal Reasoning (Complex Legal Application)",
  "capabilities": {
    "protocol": "A2A/1.0",
    "interaction_modes": ["synchronous", "streaming"],
    "tools_provided":
  }
}
This card allows any Purple Agent (not just ours) to understand how to play the game, fulfilling the Interoperability requirement.25.2.2 The Task ObjectTasks are sent as structured objects.task_id: UUIDcontext: "You are a Defense Attorney in the High People's Court..."input_data: The initial text from the LAiW case.constraints: "You have 5 turns. You must cite at least one article."5.3 The Scoring Engine: "Legal-F1"The most innovative aspect of Jurist-Bench is its scoring. A simple string match is insufficient for "Reasoning." We propose a composite metric, Legal-F1, calculated by the Green Agent.$$Legal\text{-}F1 = (W_1 \times Recall) + (W_2 \times Grounding) + (W_3 \times Logic)$$5.3.1 Component 1: Fact Recall ($W_1 = 0.3$)Mechanism: The Green Agent extracts Named Entities (Dates, Names, Locations) from the Purple Agent's final submission.Comparison: It compares these against the Element Recognition tags in the LAiW ground truth.3Goal: Ensures the agent isn't hallucinating facts.5.3.2 Component 2: Statutory Grounding ($W_2 = 0.3$)Mechanism: The Green Agent extracts citation patterns (e.g., "Article X").Comparison: It calculates the Intersection over Union (IoU) against the Legal Article Recommendation ground truth in LAiW.Goal: Ensures the agent found the correct law.5.3.3 Component 3: Logical Alignment ($W_3 = 0.4$)Mechanism: This is the "AI Judge." The Green Agent uses a quantized version of qwen/qwen3-4b-thinking-2507 (or calls an API if permitted by the host environment) to perform a Semantic Similarity check between the Purple Agent's reasoning and the Judicial Reasoning Generation text from the AC-NLG dataset.Prompt: "Compare these two legal arguments. Do they rely on the same premises to reach the conclusion? Score similarity 0-1."Goal: Measures the quality of the argument, not just the keywords.6. Implementation Roadmap: From Concept to SubmissionThis roadmap provides a step-by-step execution plan to ensure delivery by the Phase 1 deadline (January 15, 2026).2Phase 1: Data Engineering (Weeks 1-2)Objective: Create the Jurist.db database.Action:Clone Dai-shen/LAiW.Write parse_laiw.py to ingest AC-NLG.json and CJRC.json.Implement the "Information Gap" logic: split each case into public_initial, private_evidence, and ground_truth.Store in SQLite.Deliverable: A cleaned, structured database of 500 high-quality complex cases.Phase 2: Green Agent Core (Weeks 3-4)Objective: Build the A2A Server.Action:Initialize FastAPI project.Implement POST /message endpoint handling JSON-RPC.Implement the State Machine logic (Turn counting, Tool dispatching).Implement the AgentCard endpoint.Deliverable: A running server that can "talk" to a dummy client.Phase 3: Purple Agent & Integration (Weeks 5-6)Objective: Build the Baseline Competitor.Action:Install qwen_agent and vLLM.Configure qwen/qwen3-4b-thinking-2507 with enable_thinking=True.Define the tools search_law_database and request_case_file in the Qwen-Agent format.17Connect Purple to Green. Debug the A2A handshake.Deliverable: A video showing qwen/qwen3-4b-thinking-2507 asking for evidence and receiving it from Jurist-Bench.Phase 4: Evaluation & Refinement (Week 7)Objective: Tune the Difficulty.Action:Run the Purple Agent on the full 500 cases.Analyze the "Legal-F1" scores.Calibration: If scores are too high (100%), the task is too easy. Remove some metadata from the initial report to force more investigation. If too low, add hints.Deliverable: A balanced benchmark where a "blind" agent fails and the "thinking" agent succeeds.Phase 5: Submission Packaging (Week 8)Objective: Final Polish.Action:Write the Dockerfile. Ensure it builds FROM scratch or standard base images without requiring auth.Record the 3-minute Demo Video.2 Focus on the "Thinking" process and the dynamic retrieval of evidence.Write the Abstract.Deliverable: Final submission to AgentBeats.7. Strategic Insights and Winning NarrativeTo truly distinguish this entry, the final report must articulate the "Why"—the deeper insights that justify the architectural choices.7.1 Insight: Process over OutcomeMost existing benchmarks suffer from "Goodhart's Law"—when a measure becomes a target, it ceases to be a good measure. By measuring only the final verdict, models can game the system using statistical correlations (e.g., "Most murder cases end in guilty"). Jurist-Bench innovates by measuring the Investigative Trajectory. By tracking which tools the agent used, we can distinguish between a lucky guess and a reasoned conclusion. This directly addresses the competition's call for "Meaningful" evaluation.27.2 Insight: Dynamic Difficulty Adjustment (DDA)Because Jurist-Bench is a software service and not a static file, it enables Dynamic Difficulty. The Green Agent can theoretically fuzz the names or dates in the case file at runtime. This prevents "Data Contamination" (where the model memorized the case from its pre-training data). If the Green Agent changes "Mr. Zhang" to "Mr. Li" in the private evidence, and the Purple Agent still refers to "Mr. Zhang," we catch a hallucination/memorization error instantly. This feature should be highlighted in the submission abstract as a major innovation in Benchmark Integrity.7.3 Insight: The "Agent Safety" AngleSponsorship by Lambda focuses on "Agent Safety".2 Legal Hallucination is a massive safety risk (as seen in the real-world Mata v. Avianca case where ChatGPT cited fake cases). Jurist-Bench acts as a Safety Gym for legal agents. By enforcing strict grounding checks (the $W_2$ Statutory Grounding score), this benchmark explicitly rewards safe, verifiable behavior and penalizes creative fabrication. We position Jurist-Bench not just as a performance benchmark, but as a Safety Compliance Tool.8. ConclusionThe "Jurist-Bench" proposal represents a mathematically rigorous, architecturally sound, and strategically aligned plan to win Phase 1 of AgentX AgentBeats. It satisfies the core requirement of building a Green Agent while leveraging the specific strengths of the LAiW dataset (complexity) and qwen/qwen3-4b-thinking-2507 (reasoning).By shifting the focus from static NLP tasks to dynamic, stateful agent simulations, Jurist-Bench fills a critical gap in the evaluation landscape. It offers a standardized, reproducible, and challenging environment that pushes the boundaries of what Legal AI can achieve. The robust implementation of the A2A protocol ensures interoperability, while the novel Legal-F1 scoring metric ensures that the evaluation is nuanced and meaningful.The path to victory lies in the execution of the "Information Gap" transformation—turning the passive reading of case files into the active pursuit of justice. This is the essence of Agentic AI, and Jurist-Bench is the stadium where this capability will be proven.